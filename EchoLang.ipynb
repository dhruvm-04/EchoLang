{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSKzssEWSmQqnkQjx2qzjC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvm-04/EchoLang/blob/main/EchoLang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "---\n",
        "**Real-Time Speech-to-Text (STT) Translation and Transcription Tool**\n",
        "\n",
        "A system designed to perform real-time transcription and translation of code-switched speech involving a mix of Hindi-English (Hinglish) and Tamil-English into **English text**.\n",
        "\n",
        "### Core Objectives:\n",
        "- Transcribe and translate speech that includes a mixture of regional Indian languages (Hindi, Tamil) and English.\n",
        "- Ensure high accuracy in handling code-switched and accented speech.\n",
        "\n",
        "### Primary Use Cases:\n",
        "1. **Accessing a Yellow Page Directory**  \n",
        "   Helping blue-collar workers in Tier 2/3 Indian cities to search and access local services using voice-based interaction.\n",
        "\n",
        "2. **Automated Medical Prescription Creation**  \n",
        "   Passive observation and transcription of doctor-patient conversations (often in regional dialects) to automatically generate medical prescriptions and records, particularly suited for low-resource healthcare settings in rural and semi-urban areas.\n"
      ],
      "metadata": {
        "id": "3fzLKLSgQj40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Features\n",
        "---\n",
        "- **Input**: Multilingual Speech (Hindi / Tamil / English)  \n",
        "- **Output**: English Text  \n",
        "\n",
        "### Core:\n",
        "- Support for mixed local languages (code-switched)\n",
        "- Real-time Translation + Transcription\n",
        "- Contextual Understanding of code-switched speech\n",
        "- Robust Speech-to-Text (STT) conversion\n",
        "- Integration with both use cases (Yellow Pages and Medical Prescriptions)\n",
        "- Relatively low resource consumption for deployment in Tier 2/3 cities\n",
        "\n",
        "### More than just a translator:\n",
        "- Context-aware responses\n",
        "- Conversion of input to actionable information\n",
        "- Designed for environments with limited literacy requirements\n",
        "- Focuses on the intent behind user input, not just the literal words\n"
      ],
      "metadata": {
        "id": "soaoGfRfRTHW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfs0HQKlX3zZ",
        "outputId": "68a42ded-e808-465a-dc71-d426b6a0d2c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q pydub ffmpeg-python ipywidgets\n",
        "!apt-get -qq install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import whisper\n",
        "import base64\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Audio recording JavaScript\n",
        "RECORD_JS = '''\n",
        "const sleep = time => new Promise(resolve => setTimeout(resolve, time));\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader();\n",
        "  reader.onloadend = e => resolve(e.srcElement.result);\n",
        "  reader.readAsDataURL(blob);\n",
        "});\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "  recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });\n",
        "  chunks = [];\n",
        "  recorder.ondataavailable = e => chunks.push(e.data);\n",
        "  recorder.start();\n",
        "  await sleep(time);\n",
        "  recorder.onstop = async () => {\n",
        "    blob = new Blob(chunks, { type: 'audio/webm' });\n",
        "    text = await b2text(blob);\n",
        "    resolve(text);\n",
        "  };\n",
        "  recorder.stop();\n",
        "});\n",
        "'''\n",
        "\n",
        "def record_audio(seconds=5):\n",
        "    display(Javascript(RECORD_JS))\n",
        "    print(f\"Recording for {seconds} seconds...\")\n",
        "    audio_data = output.eval_js(f'record({seconds * 1000})')\n",
        "    header, encoded = audio_data.split(',', 1)\n",
        "    audio_bytes = base64.b64decode(encoded)\n",
        "    with open('recording.webm', 'wb') as f:\n",
        "        f.write(audio_bytes)\n",
        "    audio = AudioSegment.from_file('recording.webm')\n",
        "    audio.export('recording.wav', format='wav')\n",
        "    print(\"Audio recording complete\")\n",
        "    return 'recording.wav'\n",
        "\n",
        "def load_whisper_model(model_size=\"medium\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = whisper.load_model(model_size).to(device)\n",
        "    print(f\"Loaded {model_size} model on {device.upper()}\")\n",
        "    return model\n",
        "\n",
        "def detect_language(audio_path, model):\n",
        "    audio = whisper.load_audio(audio_path)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "    _, probs = model.detect_language(mel)\n",
        "    detected_lang = max(probs, key=probs.get)\n",
        "    print(f\"Detected language: {detected_lang}\")\n",
        "    return detected_lang\n",
        "\n",
        "def transcribe_audio(audio_path, model, language=None):\n",
        "    options = {\"fp16\": False} if model.device.type == \"cpu\" else {}\n",
        "    if language:\n",
        "        options[\"language\"] = language\n",
        "    result = model.transcribe(audio_path, **options)\n",
        "    return result[\"text\"]"
      ],
      "metadata": {
        "id": "oXqfforc6kQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure settings\n",
        "DURATION = 10  # Recording duration in seconds\n",
        "MODEL_SIZE = \"small\"\n",
        "\n",
        "# Load model once (cached for subsequent runs)\n",
        "model = load_whisper_model(MODEL_SIZE)\n",
        "\n",
        "# Record audio\n",
        "audio_path = record_audio(seconds=DURATION)\n",
        "\n",
        "# Detect language\n",
        "detected_lang = detect_language(audio_path, model)\n",
        "\n",
        "# Transcribe audio\n",
        "transcription = transcribe_audio(audio_path, model, language=detected_lang)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"TRANSCRIPTION RESULT:\")\n",
        "print(\"-\"*50)\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "hAa0FExcOtkW",
        "outputId": "3003d845-e0ee-489e-fff3-231d5c38ab26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded small model on CUDA\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep = time => new Promise(resolve => setTimeout(resolve, time));\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader();\n",
              "  reader.onloadend = e => resolve(e.srcElement.result);\n",
              "  reader.readAsDataURL(blob);\n",
              "});\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
              "  recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });\n",
              "  chunks = [];\n",
              "  recorder.ondataavailable = e => chunks.push(e.data);\n",
              "  recorder.start();\n",
              "  await sleep(time);\n",
              "  recorder.onstop = async () => {\n",
              "    blob = new Blob(chunks, { type: 'audio/webm' });\n",
              "    text = await b2text(blob);\n",
              "    resolve(text);\n",
              "  };\n",
              "  recorder.stop();\n",
              "});\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recording for 10 seconds...\n",
            "Audio recording complete\n",
            "Detected language: en\n",
            "\n",
            "--------------------------------------------------\n",
            "TRANSCRIPTION RESULT:\n",
            "--------------------------------------------------\n",
            " Random Music\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core dependencies\n",
        "!pip install torch transformers\n",
        "!pip install langdetect fasttext-langdetect\n",
        "!pip install numpy pandas matplotlib seaborn\n",
        "\n",
        "# IndicTrans2 dependencies\n",
        "!pip install sentencepiece sacremoses\n",
        "!git clone https://github.com/VarunGumma/IndicTransToolkit\n",
        "%cd IndicTransToolkit && pip install --editable . --use-pep517 && cd ..\n",
        "\n",
        "# Download language detection models\n",
        "!wget -q https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SSkv59AeBNf",
        "outputId": "74e195aa-04a2-4fa2-f5d4-3fc74aa243a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: fasttext-langdetect in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: fasttext>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from fasttext-langdetect) (0.9.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from fasttext-langdetect) (2.32.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext>=0.9.1->fasttext-langdetect) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext>=0.9.1->fasttext-langdetect) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext>=0.9.1->fasttext-langdetect) (2.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->fasttext-langdetect) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->fasttext-langdetect) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->fasttext-langdetect) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->fasttext-langdetect) (2025.6.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
            "fatal: destination path 'IndicTransToolkit' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: 'IndicTransToolkit && pip install --editable . --use-pep517 && cd ..'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from collections import defaultdict, Counter\n",
        "import unicodedata\n",
        "\n",
        "# Language code mappings for common scripts\n",
        "SCRIPT_LANGUAGE_MAP = {\n",
        "    'Devanagari': 'hi',  # Hindi\n",
        "    'Tamil': 'ta',       # Tamil\n",
        "    'Latin': 'en'        # English (default for Latin script)\n",
        "}\n",
        "\n",
        "# Common language patterns in WordPiece tokens\n",
        "LANGUAGE_TOKEN_PATTERNS = {\n",
        "    'hindi': {\n",
        "        'prefixes': ['##ा', '##ि', '##ी', '##ु', '##ू', '##े', '##ै', '##ो', '##ौ'],\n",
        "        'suffixes': ['ने', 'को', 'से', 'में', 'पर', 'का', 'की', 'के'],\n",
        "        'common_tokens': ['है', 'हैं', 'था', 'थी', 'थे', 'और', 'या', 'में', 'से']\n",
        "    },\n",
        "    'tamil': {\n",
        "        'prefixes': ['##ா', '##ி', '##ீ', '##ு', '##ூ', '##ெ', '##ே', '##ை', '##ொ', '##ோ', '##ௌ'],\n",
        "        'suffixes': ['ும்', 'அது', 'இது', 'அந்த', 'இந்த', 'ான்', 'ின்', 'ில்'],\n",
        "        'common_tokens': ['அது', 'இது', 'ும்', 'ான்', 'பண்ண', 'என்ன', 'எல்லா']\n",
        "    },\n",
        "    'english': {\n",
        "        'prefixes': ['##ing', '##ed', '##er', '##est', '##ly', '##tion', '##ness'],\n",
        "        'suffixes': ['the', 'and', 'or', 'but', 'with', 'from', 'to', 'at', 'in', 'on'],\n",
        "        'common_tokens': ['the', 'and', 'or', 'but', 'with', 'from', 'to', 'at', 'in', 'on', 'is', 'are', 'was', 'were']\n",
        "    }\n",
        "}\n",
        "\n",
        "def load_wordpiece_tokenizer(model_name=\"bert-base-multilingual-uncased\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(f\"Loaded tokenizer: {model_name}\")\n",
        "    return tokenizer\n",
        "\n",
        "def detect_unicode_script(text):\n",
        "    script_counts = defaultdict(int)\n",
        "    for char in text:\n",
        "        if char.isalpha():\n",
        "            script = unicodedata.name(char, '').split()[0] if unicodedata.name(char, '') else 'Unknown'\n",
        "            if script.startswith('DEVANAGARI'):\n",
        "                script_counts['Devanagari'] += 1\n",
        "            elif script.startswith('TAMIL'):\n",
        "                script_counts['Tamil'] += 1\n",
        "            elif script.startswith('LATIN'):\n",
        "                script_counts['Latin'] += 1\n",
        "    if not script_counts:\n",
        "        return 'Latin', 0.0\n",
        "    primary_script = max(script_counts.items(), key=lambda x: x[1])\n",
        "    total_chars = sum(script_counts.values())\n",
        "    confidence = primary_script[1] / total_chars if total_chars > 0 else 0.0\n",
        "    return primary_script[0], confidence\n",
        "\n",
        "def analyze_wordpiece_token_language(token, vocab_patterns=LANGUAGE_TOKEN_PATTERNS):\n",
        "    language_scores = defaultdict(float)\n",
        "    for lang, patterns in vocab_patterns.items():\n",
        "        for prefix in patterns.get('prefixes', []):\n",
        "            if token.startswith(prefix):\n",
        "                language_scores[lang] += 0.8\n",
        "        for suffix in patterns.get('suffixes', []):\n",
        "            if token.endswith(suffix):\n",
        "                language_scores[lang] += 0.7\n",
        "        if token in patterns.get('common_tokens', []):\n",
        "            language_scores[lang] += 1.0\n",
        "    if token.strip('##'):\n",
        "        script, script_confidence = detect_unicode_script(token)\n",
        "        if script in SCRIPT_LANGUAGE_MAP:\n",
        "            lang_code = SCRIPT_LANGUAGE_MAP[script]\n",
        "            lang_map = {'hi': 'hindi', 'ta': 'tamil', 'en': 'english'}\n",
        "            if lang_code in lang_map:\n",
        "                language_scores[lang_map[lang_code]] += script_confidence * 0.9\n",
        "    total_score = sum(language_scores.values())\n",
        "    if total_score > 0:\n",
        "        language_scores = {lang: score / total_score for lang, score in language_scores.items()}\n",
        "    return dict(language_scores)\n",
        "\n",
        "def wordpiece_language_detection(text, tokenizer):\n",
        "    encoded = tokenizer(text, add_special_tokens=False, return_tensors='pt')\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
        "    sentence_language_scores = defaultdict(float)\n",
        "    for token in tokens:\n",
        "        if token in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:\n",
        "            continue\n",
        "        token_lang_scores = analyze_wordpiece_token_language(token)\n",
        "        token_weight = len(token.replace('##', '')) / 10.0 + 0.1\n",
        "        for lang, score in token_lang_scores.items():\n",
        "            sentence_language_scores[lang] += score * token_weight\n",
        "    total_sentence_score = sum(sentence_language_scores.values())\n",
        "    if total_sentence_score > 0:\n",
        "        sentence_language_scores = {\n",
        "            lang: score / total_sentence_score\n",
        "            for lang, score in sentence_language_scores.items()\n",
        "        }\n",
        "    result = {\n",
        "        'original_text': text,\n",
        "        'wordpiece_tokens': tokens,\n",
        "        'token_count': len(tokens),\n",
        "        'sentence_languages': dict(sentence_language_scores),\n",
        "        'dominant_language': max(sentence_language_scores.items(), key=lambda x: x[1])[0] if sentence_language_scores else 'unknown'\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def format_wordpiece_analysis(analysis_result):\n",
        "    result = analysis_result\n",
        "    output = []\n",
        "    output.append(f\"Text: {result['original_text']}\")\n",
        "    output.append(f\"WordPiece Tokens ({result['token_count']}): {' | '.join(result['wordpiece_tokens'])}\")\n",
        "    output.append(\"\")\n",
        "    output.append(\"Sentence-Level Language Distribution:\")\n",
        "    for lang, score in sorted(result['sentence_languages'].items(), key=lambda x: x[1], reverse=True):\n",
        "        percentage = score * 100\n",
        "        output.append(f\"  {lang.title()}: {percentage:.1f}%\")\n",
        "    output.append(f\"Dominant Language: {result['dominant_language'].title()}\")\n",
        "    return \"\\n\".join(output)\n",
        "\n",
        "def batch_wordpiece_analysis(texts, tokenizer):\n",
        "    results = []\n",
        "    for i, text in enumerate(texts, 1):\n",
        "        print(f\"Analyzing text {i}/{len(texts)}...\")\n",
        "        result = wordpiece_language_detection(text, tokenizer)\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "def generate_language_statistics(results):\n",
        "    all_languages = set()\n",
        "    token_counts = []\n",
        "    dominant_languages = []\n",
        "    for result in results:\n",
        "        all_languages.update(result['sentence_languages'].keys())\n",
        "        token_counts.append(result['token_count'])\n",
        "        dominant_languages.append(result['dominant_language'])\n",
        "    stats = {\n",
        "        'total_texts': len(results),\n",
        "        'unique_languages_detected': len(all_languages),\n",
        "        'languages_detected': sorted(list(all_languages)),\n",
        "        'average_tokens_per_text': np.mean(token_counts),\n",
        "        'dominant_language_distribution': dict(Counter(dominant_languages))\n",
        "    }\n",
        "    return stats\n"
      ],
      "metadata": {
        "id": "sSR7SCOcepwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "SAMPLE_TEXTS = [\n",
        "    \"Hi doctor, मेरा पेट दर्द हो रहा है, can you help pannunga?\",\n",
        "    \"Tomorrow class है क्या? நான் calendar check pannala.\",\n",
        "    \"मैंने homework finish कर लिया, now I'm going to play cricket with nanban.\",\n",
        "    \"இந்த book super है, I read it last night before सोने गया।\",\n",
        "    \"Please come early, क्योंकि हमें बस पकड़नी है, illa late aagidum.\",\n",
        "    \"I don't understand ये वाला concept, teacher kitte केलुंगो.\",\n",
        "    \"Naan lunch skip pannitten, अभी मुझे बहुत भूख लगी है, let's eat?\",\n",
        "    \"He is not feeling well, इसलिए आज स्कूल नहीं आया, avar rest eduthukaraaru.\",\n",
        "    \"My sister exam के लिए पढ़ रही है, அவள் ரொம்ப nervous-ஆ இருக்கா.\",\n",
        "    \"चलो चलते हैं, bus stop के पास milalaam, athu nalla idea.\"\n",
        "]\n",
        "\n",
        "# Load WordPiece tokenizer\n",
        "print(\"Loading BERT Multilingual WordPiece Tokenizer...\")\n",
        "print(\"=\" * 60)\n",
        "tokenizer = load_wordpiece_tokenizer()\n",
        "print()\n",
        "\n",
        "# Perform WordPiece-based language analysis\n",
        "print(\"WordPiece-Based Language Detection Analysis\")\n",
        "print(\"=\" * 60)\n",
        "all_results = batch_wordpiece_analysis(SAMPLE_TEXTS, tokenizer)\n",
        "\n",
        "# Display sentence-level analysis for each text\n",
        "for i, result in enumerate(all_results, 1):\n",
        "    print(f\"Analysis {i}:\")\n",
        "    print(\"-\" * 40)\n",
        "    formatted_analysis = format_wordpiece_analysis(result)\n",
        "    print(formatted_analysis)\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Generate and display summary statistics\n",
        "print(\"\\nLanguage Detection Statistics:\")\n",
        "print(\"-\" * 40)\n",
        "stats = generate_language_statistics(all_results)\n",
        "\n",
        "print(f\"Total texts analyzed: {stats['total_texts']}\")\n",
        "print(f\"Unique languages detected: {stats['unique_languages_detected']}\")\n",
        "print(f\"Languages found: {', '.join([lang.title() for lang in stats['languages_detected']])}\")\n",
        "print(f\"Average WordPiece tokens per text: {stats['average_tokens_per_text']:.1f}\")\n",
        "print()\n",
        "\n",
        "print(\"Dominant Language Distribution:\")\n",
        "for lang, count in sorted(stats['dominant_language_distribution'].items(), key=lambda x: x[1], reverse=True):\n",
        "    percentage = (count / stats['total_texts']) * 100\n",
        "    print(f\"  {lang.title()}: {count} texts ({percentage:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9gwwqoBe6xF",
        "outputId": "fa2faf72-dcac-4e1b-8071-23cbb116242f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT Multilingual WordPiece Tokenizer...\n",
            "============================================================\n",
            "Loaded tokenizer: bert-base-multilingual-uncased\n",
            "\n",
            "WordPiece-Based Language Detection Analysis\n",
            "============================================================\n",
            "Analyzing text 1/10...\n",
            "Analyzing text 2/10...\n",
            "Analyzing text 3/10...\n",
            "Analyzing text 4/10...\n",
            "Analyzing text 5/10...\n",
            "Analyzing text 6/10...\n",
            "Analyzing text 7/10...\n",
            "Analyzing text 8/10...\n",
            "Analyzing text 9/10...\n",
            "Analyzing text 10/10...\n",
            "Analysis 1:\n",
            "----------------------------------------\n",
            "Text: Hi doctor, मेरा पेट दर्द हो रहा है, can you help pannunga?\n",
            "WordPiece Tokens (20): hi | doctor | , | म | ##रा | प | ##ट | दर | ##द | हो | रहा | ह | , | can | you | help | pan | ##nung | ##a | ?\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  English: 59.6%\n",
            "  Hindi: 40.4%\n",
            "Dominant Language: English\n",
            "============================================================\n",
            "Analysis 2:\n",
            "----------------------------------------\n",
            "Text: Tomorrow class है क्या? நான் calendar check pannala.\n",
            "WordPiece Tokens (12): tomorrow | class | ह | कया | ? | ந | ##ான | calendar | check | panna | ##la | .\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  English: 78.0%\n",
            "  Hindi: 12.0%\n",
            "  Tamil: 10.0%\n",
            "Dominant Language: English\n",
            "============================================================\n",
            "Analysis 3:\n",
            "----------------------------------------\n",
            "Text: मैंने homework finish कर लिया, now I'm going to play cricket with nanban.\n",
            "WordPiece Tokens (19): मन | home | ##work | finish | कर | लिया | , | now | i | ' | m | going | to | play | cricket | with | nan | ##ban | .\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  English: 84.5%\n",
            "  Hindi: 15.5%\n",
            "Dominant Language: English\n",
            "============================================================\n",
            "Analysis 4:\n",
            "----------------------------------------\n",
            "Text: இந்த book super है, I read it last night before सोने गया।\n",
            "WordPiece Tokens (15): இநத | book | super | ह | , | i | read | it | last | night | before | स | ##ोन | गया | ।\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  English: 72.2%\n",
            "  Hindi: 20.4%\n",
            "  Tamil: 7.4%\n",
            "Dominant Language: English\n",
            "============================================================\n",
            "Analysis 5:\n",
            "----------------------------------------\n",
            "Text: Please come early, क्योंकि हमें बस पकड़नी है, illa late aagidum.\n",
            "WordPiece Tokens (18): please | come | early | , | कयोकि | हम | बस | प | ##कड | ##नी | ह | , | illa | late | aa | ##gi | ##dum | .\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  English: 63.3%\n",
            "  Hindi: 36.7%\n",
            "Dominant Language: English\n",
            "============================================================\n",
            "Analysis 6:\n",
            "----------------------------------------\n",
            "Text: I don't understand ये वाला concept, teacher kitte केलुंगो.\n",
            "WordPiece Tokens (16): i | don | ' | t | understand | य | वाला | concept | , | teacher | kit | ##te | कल | ##ग | ##ो | .\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  English: 75.0%\n",
            "  Hindi: 25.0%\n",
            "Dominant Language: English\n",
            "============================================================\n",
            "Analysis 7:\n",
            "----------------------------------------\n",
            "Text: Naan lunch skip pannitten, अभी मुझे बहुत भूख लगी है, let's eat?\n",
            "WordPiece Tokens (24): naa | ##n | lunch | skip | pan | ##nit | ##ten | , | अ | ##भी | म | ##झ | बहत | भ | ##ख | लग | ##ी | ह | , | let | ' | s | eat | ?\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  English: 61.9%\n",
            "  Hindi: 38.1%\n",
            "Dominant Language: English\n",
            "============================================================\n",
            "Analysis 8:\n",
            "----------------------------------------\n",
            "Text: He is not feeling well, इसलिए आज स्कूल नहीं आया, avar rest eduthukaraaru.\n",
            "WordPiece Tokens (22): he | is | not | feeling | well | , | इसलिए | आज | सकल | नही | आय | ##ा | , | ava | ##r | rest | edu | ##th | ##uka | ##raa | ##ru | .\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  English: 70.3%\n",
            "  Hindi: 29.7%\n",
            "Dominant Language: English\n",
            "============================================================\n",
            "Analysis 9:\n",
            "----------------------------------------\n",
            "Text: My sister exam के लिए पढ़ रही है, அவள் ரொம்ப nervous-ஆ இருக்கா.\n",
            "WordPiece Tokens (25): my | sister | ex | ##am | क | लिए | प | ##ढ | रही | ह | , | அ | ##வ | ##ள | ர | ##ெ | ##ாம | ##ப | nervous | - | ஆ | இரு | ##கக | ##ா | .\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  Tamil: 39.4%\n",
            "  English: 36.4%\n",
            "  Hindi: 24.2%\n",
            "Dominant Language: Tamil\n",
            "============================================================\n",
            "Analysis 10:\n",
            "----------------------------------------\n",
            "Text: चलो चलते हैं, bus stop के पास milalaam, athu nalla idea.\n",
            "WordPiece Tokens (20): चल | ##ो | चल | ##त | ह | , | bus | stop | क | पास | mila | ##laa | ##m | , | at | ##hu | nal | ##la | idea | .\n",
            "\n",
            "Sentence-Level Language Distribution:\n",
            "  English: 67.9%\n",
            "  Hindi: 32.1%\n",
            "Dominant Language: English\n",
            "============================================================\n",
            "\n",
            "Language Detection Statistics:\n",
            "----------------------------------------\n",
            "Total texts analyzed: 10\n",
            "Unique languages detected: 3\n",
            "Languages found: English, Hindi, Tamil\n",
            "Average WordPiece tokens per text: 19.1\n",
            "\n",
            "Dominant Language Distribution:\n",
            "  English: 9 texts (90.0%)\n",
            "  Tamil: 1 texts (10.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7rSBgW2iA7R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}